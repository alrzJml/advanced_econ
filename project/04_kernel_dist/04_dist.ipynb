{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation of ECM model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importig Two helper functions from `./helper_functions`:\n",
    "\n",
    "- `stock_list`: This functions gets an index name (e.g. 'Dow Jones', 'CAC 40', 'DAX', 'Teh50') and returns the list of stocks in that index.\n",
    "\n",
    "- `stock_prices`: This functions recieves a list of tickers and returns a pandas dataframe containing prices of the corresponding tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import stock_prices, stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM, select_coint_rank, select_order\n",
    "\n",
    "from helper_functions import stock_prices, stock_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always better to have a large sample while performing hypothesis testing. So the sample size has been increased from 521 in previous project (i.e. `01_pair_trading`) to 720."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the list of cointegrated tickers that have been calculated in the 1st project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "file = pd.ExcelFile('../01_pair_trading/pairs_2023-01-15.xlsx')\n",
    "sheet_names = ['Dow Jones', 'CAC 40', 'Dax', 'Teh50']\n",
    "for sheet in sheet_names:\n",
    "    df_tmp = pd.read_excel(file, sheet_name=sheet)\n",
    "    df = pd.concat([df, df_tmp])\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same cointegration function as was in the first project (i.e. `01_pair_trading`).\n",
    "\n",
    "We need to test for cointegration for each pair again, because the sample size has increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cointegration_params(df, verbose=False):\n",
    "    lag_order = select_order(df, maxlags=10, deterministic=\"ci\")\n",
    "    lag_order = lag_order.aic\n",
    "\n",
    "    rank_test = select_coint_rank(df, 0, lag_order, method=\"trace\",\n",
    "                              signif=0.05)\n",
    "\n",
    "    is_cointegrated = rank_test.test_stats[0] > rank_test.crit_vals[0]\n",
    "    if verbose:\n",
    "        print(rank_test.summary())\n",
    "    if not is_cointegrated:\n",
    "        return False, np.NaN, np.NAN\n",
    "    \n",
    "    model = VECM(df, deterministic=\"ci\",\n",
    "             k_ar_diff=lag_order,\n",
    "             coint_rank=rank_test.rank)\n",
    "    vecm_res = model.fit()\n",
    "\n",
    "    return True, vecm_res.beta, vecm_res.const_coint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function Will convert the arabic glyphs to standard farsi glyphs. This will be helpful while looking Tehran50 tickers up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groom(s):\n",
    "    s = s.replace('ي', 'ی')\n",
    "    s = s.replace('ك', 'ک')\n",
    "    return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normality, we perform 5 tests.\n",
    "\n",
    "1. jarque_bera\n",
    "2. anderson\n",
    "3. cramervonmises\n",
    "4. lilliefors\n",
    "5. Kolmogorov-Smirnov\n",
    "\n",
    "The last one will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "\n",
    "def is_normal_jb(x) -> bool:\n",
    "    test = stats.jarque_bera(x)\n",
    "    return test.pvalue > 0.05\n",
    "\n",
    "def is_normal_ad(x) -> bool:\n",
    "    test = stats.anderson(x)\n",
    "    return test.statistic < test.critical_values[2]\n",
    "\n",
    "def is_normal_crm(x) -> bool:\n",
    "    test = stats.cramervonmises(x, 'norm')\n",
    "    return test.pvalue > 0.05\n",
    "\n",
    "def is_normal_lil(x) -> bool:\n",
    "    test = lilliefors(x,  dist='norm')\n",
    "    return test[1] > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function receives a series and a distribution name and performs a Kolmogorov-Smirnov test. If returns the test result and the parameters that best fits the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def ks_test(data, dist_name, p_val_tresh=0.01):\n",
    "    y, x = np.histogram(data, bins=100, density=True)\n",
    "    x = [(this + x[i + 1]) / 2.0 for i, this in enumerate(x[0:-1])]\n",
    "\n",
    "    dist = eval(\"scipy.stats.\"+ dist_name)\n",
    "    if (dist_name == \"nbinom\"):\n",
    "        p = np.mean(data)/(np.std(data)**2)\n",
    "        n = np.mean(data)*p/(1.0 - p)\n",
    "        if n<0 or p<0 or p>1:\n",
    "            return True, np.nan, np.nan, np.nan\n",
    "        param = (n, p)\n",
    "    else:\n",
    "        param = dist.fit(data)\n",
    "\n",
    "    dist_fitted = dist(*param)\n",
    "\n",
    "    ks_stat, ks_pval = stats.kstest(data, dist_fitted.cdf)\n",
    "    return (ks_pval < p_val_tresh), dist, param, x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We implenet this test for normal and other distributions at the same time.\n",
    "\n",
    "The Logic is as follows:\n",
    "\n",
    "1. For a series, we find the best parameters for each distribution that fits the data best.\n",
    "2. We perform the `kstest` on the data and the distribution.\n",
    "3. If the null hypothesis rejected, Then we can conclude that the distribution doesn't fit the data well.\n",
    "4. If the null hypothesis is not rejected, We can loosely conclude that the distribuiton fits the data well enough and we save it for the plotting process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_dist(data, ticker1, ticker2, indice_path):\n",
    "\n",
    "    fitted_normal_methods = []\n",
    "    fitted_dists = []\n",
    "\n",
    "    normal_methods = [\"jb\", \"ad\", \"crm\", \"lil\"]\n",
    "    for method in normal_methods:\n",
    "        fn = eval(f\"is_normal_{method}\")\n",
    "        if fn(data):\n",
    "            fitted_normal_methods.append(method)\n",
    "        \n",
    "\n",
    "    options = [\"norm\", \"lognorm\", \"chi2\", \"t\", \"beta\", \"gamma\", \"weibull_min\", \"nbinom\"]\n",
    "\n",
    "    hs = plt.hist(data, bins=80, density=True, label=\"data\");\n",
    "    for dist_name in options:\n",
    "        is_h0_rejected, dist,  param, x =  ks_test(data, dist_name)\n",
    "        if is_h0_rejected:\n",
    "            continue\n",
    "        else:\n",
    "            fitted_dists.append(dist_name)\n",
    "            if dist_name == \"nbinom\":\n",
    "                h = plt.plot(x, dist.pmf(x, *param), label=dist_name);\n",
    "            else:\n",
    "                h = plt.plot(x, dist.pdf(x, *param), label=dist_name);\n",
    "\n",
    "    plt.title(f\"{ticker1} & {ticker2}\")\n",
    "    plt.legend();\n",
    "    plt.savefig(rf'{indice_path}/{ticker1} & {ticker2}.png')\n",
    "    plt.close()\n",
    "\n",
    "    return fitted_normal_methods, fitted_dists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cointegrated pair, We build the ECM model and test for normality and other distributions. Finally, we save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "PATH = r'./plots/'\n",
    "if os.path.exists(PATH):\n",
    "    shutil.rmtree(PATH)\n",
    "os.makedirs(PATH)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pairs = []\n",
    "for indice in ['Dow Jones', 'CAC 40', 'Dax', 'Teh50']:\n",
    "    indice_path = PATH + indice\n",
    "    os.makedirs(indice_path)\n",
    "    \n",
    "    print(indice, '>>', flush=True)\n",
    "    df1 = df[df['indice']==indice]\n",
    "    tickers = stock_list.get_stock_list(index=indice)\n",
    "    isTSE = (indice == 'Teh50')\n",
    "    if isTSE:\n",
    "        tickers = [groom(x) for x in tickers]\n",
    "    data_historical = stock_prices.get_prices(tickers, isTSE)\n",
    "\n",
    "    for i in range(df1.shape[0]):\n",
    "        ticker1, ticker2, indice = df1.iloc[i]\n",
    "        data_historical1 = data_historical[[ticker1, ticker2]]\n",
    "        data_historical1 = data_historical1.dropna(how='all')\n",
    "        data = data_historical1[-interval:]\n",
    "        limitPer = len(data) * .85\n",
    "        data = data.dropna(thresh=limitPer, axis=1)\n",
    "        data = np.log(data)\n",
    "        data = data.dropna(how='any')\n",
    "        cols = data.columns\n",
    "\n",
    "        for i in range(len(cols)-1):\n",
    "            for j in range(i+1, len(cols)):\n",
    "                try:\n",
    "                    is_cointegrated, BJ2n, C0J2n = get_cointegration_params(data.dropna(how='any'))\n",
    "                except:\n",
    "                    continue\n",
    "                if not is_cointegrated:\n",
    "                    continue\n",
    "                \n",
    "                ecm = np.matmul(data, BJ2n) + C0J2n\n",
    "                x = ecm[0].values\n",
    "                fitted_normal_methods, fitted_dists = test_for_dist(x, ticker1, ticker2, indice_path)\n",
    "                pairs.append({\n",
    "                    'sym1': cols[i],\n",
    "                    'sym2': cols[j],\n",
    "                    'indice': indice,\n",
    "                    'fitted_normal_methods': fitted_normal_methods,\n",
    "                    'fitted_dists': fitted_dists\n",
    "                })\n",
    "\n",
    "\n",
    "filename = rf'./ecm_dists.xlsx'\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "df_errors = pd.DataFrame(pairs)\n",
    "for index, group_df in df_errors.groupby(\"indice\"):   \n",
    "    group_df.to_excel(writer, sheet_name=str(index),index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
